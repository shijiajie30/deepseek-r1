from transformers import AutoTokenizer, AutoModelForCausalLM
from data_prepare import samples
from datasets import load_dataset
import json

# åŠ è½½å¾®è°ƒå‰çš„æ¨¡å‹
print("[1/9] å¼€å§‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")

# ç¬¬äºŒæ­¥ï¼Œåˆ¶ä½œæ•°æ®é›†
print("\n[2/9] æ­£åœ¨ç”Ÿæˆè®­ç»ƒæ•°æ®é›†...")
with open('datasets.jsonl', 'w', encoding='utf-8') as f:
    for s in samples:
        json_line = json.dumps(s, ensure_ascii=False)
        f.write(json_line + '\n')
    else:
        print("âœ… æ•°æ®é›†å·²ç”Ÿæˆè‡³ datasets.jsonl")

# ç¬¬ä¸‰æ­¥ï¼Œå‡†å¤‡è®­ç»ƒé›†å’ŒéªŒè¯é›†
print("\n[3/9] æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†...")
dataset = load_dataset('json', data_files={'train': 'datasets.jsonl'}, split='train')

train_test_split = dataset.train_test_split(test_size=0.1)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

# ç¬¬å››æ­¥ï¼Œç¼–å†™tokenizerå¤„ç†å‡½æ•°
print("\n[4/9] æ­£åœ¨è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–å¤„ç†...")
def tokenize_function(examples):
    texts = [f"prompt: {prompt}\ncompletion: {completion}" for prompt, completion in zip(examples['prompt'], examples['completion'])]
    tokens =  tokenizer(texts, padding='max_length', truncation=True, max_length=512)
    tokens['labels'] = tokens['input_ids'].copy()
    return tokens

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)
print("âœ… æ–‡æœ¬å‘é‡åŒ–å¤„ç†å®Œæˆ")

# ç¬¬äº”æ­¥ï¼Œé‡åŒ–è®¾ç½®
print("\n[5/9] æ­£åœ¨åˆå§‹åŒ–8ä½é‡åŒ–é…ç½®...")
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map='auto')

# ç¬¬å…­æ­¥ï¼Œloraå¾®è°ƒè®¾ç½®
print("\n[6/9] æ­£åœ¨é…ç½®LoRAå¾®è°ƒå‚æ•°...")
from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
   r=8,
   lora_alpha=16,
   lora_dropout=0.5,
   task_type=TaskType.CAUSAL_LM 
)
model = get_peft_model(model, lora_config)

# ç¬¬ä¸ƒæ­¥ï¼Œè®¾ç½®è®­ç»ƒå‚æ•°
print("\n[7/9] æ­£åœ¨è®¾ç½®è®­ç»ƒè¶…å‚æ•°...")
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
   output_dir='./finetuned_model',
   num_train_epochs=10,
   per_device_train_batch_size=4,
   gradient_accumulation_steps=8,
   fp16=True,
   logging_steps=10,
   save_steps=100,
   eval_strategy='steps',
   eval_steps=10,
   learning_rate=2e-5,
   logging_dir='./logs',
   run_name='finetuning_qwen1.5b'
)

trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=tokenized_train_dataset,
  eval_dataset=tokenized_test_dataset
)

# ç¬¬å…«æ­¥ï¼Œå¼€å§‹è®­ç»ƒ
print("\n[8/9] ğŸš€ å¯åŠ¨æ¨¡å‹è®­ç»ƒä»»åŠ¡...")
trainer.train()
print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")

# ç¬¬ä¹æ­¥ï¼Œä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
print("\n[9/9] æ­£åœ¨ä¿å­˜å¾®è°ƒæ¨¡å‹...")
save_path = './saved_model'
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

# é‡Šæ”¾è®­ç»ƒæ¨¡å‹å†…å­˜
import torch
del model, trainer
torch.cuda.empty_cache()

# ä¿å­˜å…¨é‡æ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–é…ç½®åŠ è½½åŸºç¡€æ¨¡å‹ï¼‰
final_save_path = './final_saved_model'
from peft import PeftModel
base_model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    quantization_config=quantization_config,  # å¤ç”¨é‡åŒ–é…ç½®
    device_map='auto'
)
model = PeftModel.from_pretrained(base_model, save_path)
model = model.merge_and_unload()

# å†æ¬¡é‡Šæ”¾å†…å­˜
del base_model
torch.cuda.empty_cache()

model.save_pretrained(final_save_path)
tokenizer.save_pretrained(final_save_path)
print(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆ (LoRAæ¨¡å‹: {save_path}, å…¨é‡æ¨¡å‹: {final_save_path})")